{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Machine Learning and Data Analytics for Industry 4.0 | SS2020</h1>\n",
    "    <hr>\n",
    "    <br>\n",
    "    <div style=\"margin:0 60px 0px 0\">\n",
    "        <img src=\"https://www.mad.tf.fau.de/files/2019/04/logo_mad.png\" align=center width=500>\n",
    "    </div>\n",
    "    <h2>Generation of Real Looking Images Using GANs</h2>\n",
    "    <h3> by Timo Bohnstedt</h3>\n",
    "    <h3> Supervisor: Franz Köferl, M. Sc.</h3>\n",
    "    <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "\n",
    "### Outline for this Notebook `(clickable)`\n",
    "\n",
    "1. [Data Understanding](#1) <br>\n",
    "1.1. [Loading the MNIST Dataset](#1.1) <br>\n",
    "2. [Data Preparation](#2) <br>\n",
    "2.1. [Helper Methods](#1.2) <br>\n",
    "3. [Modeling](#3) <br>\n",
    "3.1. [Deep Convolutional Generative Adversarial Networks (DCGAN)](#3.1) <br>\n",
    "3.2. [Conditional Deep Convolutional Generative Adversarial Networks (cDCGAN)](#3.2) <br>\n",
    "4. [Evaluation](#4) <br>\n",
    "4.2. [Visual Examination](#4.1) <br>\n",
    "4.2. [K-Nearest-Neighbour (kNN)](#4.2) <br>\n",
    "4.3. [Fréchet Inception Distance (FID)](#4.3) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation of the Notebook\n",
    "\n",
    "First we need to initialize Python.  Run the below cells to import the neccesary packages and set some global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "import cv2\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import IPython.core.display \n",
    "\n",
    "# To generate GIFs\n",
    "!pip install -q imageio\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "random.seed(100)\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "\n",
    "## 1. Data Understanding\n",
    "\n",
    "<a name='1.1'></a>\n",
    "\n",
    "### 1.1. Loading the MNIST Dataset\n",
    "The dataset I am using is called “MNIST database”. Here is the official description, quoted from the [website](http://yann.lecun.com/exdb/mnist/)\n",
    ">The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "The figure below shows a few examples from the dataset provided by the [website](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "\n",
    "   <div style=\"margin:0 60px 0px 0\">\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" align=center>\n",
    "    </div>\n",
    "\n",
    "To start with the task, I used the Keras-API to get the labels. The Keras datasets provide the opportunity to load and preprocess the data at once. To start with the task, I used the Keras-API to get the labels. The Keras datasets provide the opportunity to load and preprocess the data at once. So the data will be downloaded at first, scaled and normalized. Afterwards, it gets divided into distinct parts for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "train_labels =  tf.keras.utils.to_categorical(train_labels, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Machine Learning models tuning is a type of optimization problem. We have a set of hyperparameters, and we aim to find the right combination of their values which can help us to see either the minimum or the maximum. Tuning hyperparameters was left out within this project.  This is why the hyperparameters are hard-coded such the network can be trained on the following values. \n",
    "\n",
    "\n",
    ">The values are taken from the original paper suggested by Phillip Isola and his colleagues in their paper [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "buffer_size = 60000\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "noise_dim = 100\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(batch_size)\n",
    "ACTIVATION = tf.keras.layers.Activation(\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "<a name='2'></a>\n",
    "\n",
    "# 2. Data Preparation\n",
    "\n",
    "<a name='2.1'></a>\n",
    "\n",
    "### 2.1. Helper Methods\n",
    "\n",
    "As mentioned in the data loading part, Keras-API offers an opportunity to preprocess the data. Nevertheless, a few helper methods are useful when it comes to present and evaluate the results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnist_image_grid_for_dcgan(generator, title: str = \"Generated images\"):\n",
    "    \"\"\"Form an image grid with gnerated images by using the generator of a GAN.\n",
    "\n",
    "    Keyword arguments:\n",
    "    generator -- the generator model of any kind GAN\n",
    "    imag -- the imaginary part (default 0.0)\n",
    "    \"\"\"\n",
    "    generated_images = []\n",
    "\n",
    "    for i in range(10):\n",
    "        noise = generate_noise((10, 100))        \n",
    "        gen_images = generator.predict(noise, verbose=0)\n",
    "        generated_images.extend(gen_images)\n",
    "\n",
    "    generated_images = np.array(generated_images)\n",
    "    image_grid = combine_images(generated_images)\n",
    "    image_grid = inverse_transform_images(image_grid)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(image_grid, cmap=\"gray\")\n",
    "    ax.set_title(title)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    image = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_images_for_advanced_evaluation(generator, examples):\n",
    "    \"\"\"Generates a set of images while using the generator part of a GAN.\n",
    "\n",
    "    Keyword arguments:\n",
    "    generator -- the generator model of simple GAN\n",
    "    examples -- a batch of real images in order to compare it with the genrated examples\n",
    "    \"\"\"\n",
    "    generated_images = []\n",
    "\n",
    "    for i in range(examples):\n",
    "        noise = generate_noise((examples, 100))        \n",
    "        gen_images = generator.predict(noise, verbose=0)\n",
    "        generated_images.extend(gen_images)\n",
    "\n",
    "    generated_images = np.array(generated_images)\n",
    "    return generated_images\n",
    "\n",
    "def get_images_for_advanced_evaluation_cdcgan(generator, examples):\n",
    "    \"\"\"Generates a set of images while using the generator part of a GAN.\n",
    "\n",
    "    Keyword arguments:\n",
    "    generator -- the generator model of cGAN\n",
    "    examples -- a batch of real images in order to compare it with the genrated examples\n",
    "    \"\"\"\n",
    "    generated_images = []\n",
    "\n",
    "    for i in range(examples):\n",
    "        noise = generate_noise((10, 100))\n",
    "        label_input = generate_condition_embedding(i, 10)\n",
    "        gen_images = generator.predict([noise, label_input], verbose=0)\n",
    "        generated_images.extend(gen_images)\n",
    "        \n",
    "    generated_images = np.array(generated_images)\n",
    "    return generated_images\n",
    "    return generated_images\n",
    "\n",
    "\n",
    "def save_generated_image(image, epoch, iteration, folder_path):\n",
    "    \"\"\"Save images on your hard drive\n",
    "\n",
    "    Keyword arguments:\n",
    "    image -- the image which shall save on your hard drive\n",
    "    epoch -- string  contains  the epoch in which the image was generated in order generate a unquie name\n",
    "    itteration -- string contains the iteration in which the image was generated in order generate a unquie name\n",
    "    folder_path -- string contains the path on your local hard drive on which the image shall be stored\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    file_path = \"{0}/{1}_{2}.png\".format(folder_path, epoch, iteration)\n",
    "    cv2.imwrite(file_path, image.astype(np.uint8))\n",
    "    \n",
    "def generate_noise(shape: tuple):\n",
    "    \"\"\"Generate noise to feed it into the generator of any GAN as an input z.\n",
    "\n",
    "    Keyword arguments:\n",
    "    shape -- Defines the actual size of the noisy image and must have the same size as defined in in the input section of the GAN.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(0, 1, size=shape)\n",
    "    return noise\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    \"\"\"From a grid of images.\n",
    "\n",
    "    Keyword arguments:\n",
    "    generated_images -- images generated by a GAN.\n",
    "    \"\"\"\n",
    "    num_images = generated_images.shape[0]\n",
    "    new_width = int(math.sqrt(num_images))\n",
    "    new_height = int(math.ceil(float(num_images) / new_width))\n",
    "    grid_shape = generated_images.shape[1:3]\n",
    "    grid_image = np.zeros((new_height * grid_shape[0], new_width * grid_shape[1]), dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / new_width)\n",
    "        j = index % new_width\n",
    "        grid_image[i * grid_shape[0]:(i + 1) * grid_shape[0], j * grid_shape[1]:(j + 1) * grid_shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return grid_image\n",
    "\n",
    "def inverse_transform_images(images: np.ndarray):\n",
    "    \"\"\"From the [-1, 1] range transform the images back to [0, 255].\n",
    "    \n",
    "    Keyword arguments:\n",
    "    images -- image from with colour range of [-1,1]\n",
    "    \"\"\"\n",
    "\n",
    "    images = images * 127.5 + 127.5\n",
    "    images = images.astype(np.uint8)\n",
    "    return images\n",
    "\n",
    "def generate_images_and_labels(generator, nb_images: int, label: int):\n",
    "    \"\"\"Generates images an their labels by example while using the generator part of a GAN.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    images -- image from with colour range of [-1,1]\n",
    "    \"\"\"\n",
    "    noise = generate_noise((nb_images, 100))\n",
    "    label_batch = generate_condition_embedding(label, nb_images)\n",
    "    generated_images = generator.predict([noise, label_batch], verbose=0)\n",
    "    return generated_images\n",
    "\n",
    "def generate_condition_embedding(label: int, nb_of_label_embeddings: int):\n",
    "    \"\"\"Transform labels into a vector emedding in order to use it whin a cDCGAN implementaion.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    label -- inter which defines the label which corresponds with an input image of GAN\n",
    "    nb_of_label_embeddings -- interger which defines the total number of vectors which shall generated\n",
    "    \"\"\"\n",
    "    label_embeddings = np.zeros((nb_of_label_embeddings, 100))\n",
    "    label_embeddings[:, label] = 1\n",
    "    return label_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "GANs and cGANs stipulate a high-level framework with a lot of freedom in designing its various components. The prototype which will used in order to find answers to the research question uses a so-called conditional deep convolutional neural network (cDCGAN) and uses the hyperparameters suggested by Phillip Isola and his colleagues in their paper [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004.pdf). More precisely the bat size is 100, the learning rate 0.0002, the amount of training epochs is 100. Furthermore, it uses uses the Adam optimizer. \n",
    "\n",
    "To set a ground through within the evaluation a more simple GAN will be implemented as well. \n",
    "\n",
    "<a name='3.1'></a>\n",
    "\n",
    "### 3.1. Deep Convolutional Generative Adversarial Networks (DCGAN)\n",
    " \n",
    "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). \n",
    "Start with a `Dense` layer that takes this seed as input, then upsample several times \n",
    "until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "def make_generator_model():\n",
    "    \"\"\"Using the Keras-API and Tensorlfow as\n",
    "    Backend to define a generator model for the GAN\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check if the generator genertae an example. It must show a noisy image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl7jcC7TdPTG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12544)             1254400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f87487a1210>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 251.565 248.518125 \n",
       "L 251.565 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 244.365 224.64 \n",
       "L 244.365 7.2 \n",
       "L 26.925 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#pbddecd712b)\">\n",
       "    <image height=\"218\" id=\"image4f7902a132\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m2617ab9422\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m2617ab9422\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m2617ab9422\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m2617ab9422\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m2617ab9422\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m2617ab9422\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m2617ab9422\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m084b780802\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m084b780802\" y=\"11.082857\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m084b780802\" y=\"49.911429\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m084b780802\" y=\"88.74\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m084b780802\" y=\"127.568571\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m084b780802\" y=\"166.397143\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m084b780802\" y=\"205.225714\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 26.925 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 244.365 224.64 \n",
       "L 244.365 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 244.365 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 26.925 7.2 \n",
       "L 244.365 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pbddecd712b\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "generator = make_generator_model()\n",
    "generator.summary()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "def make_discriminator_model():\n",
    "    \"\"\"Using the Keras-API and Tensorlfow as Backend to define a discriminator model for the GAN\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)\n",
    "\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"Generates a metric which tells \n",
    "    the sucess of the discriminator in respect too the generator.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    real_input -- sample batch of the training data\n",
    "    fake_output -- sample of generated images\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    \"\"\"Generates a metric which tells the \n",
    "    sucess of the generator in respect too the discriminator.\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    fake_output -- sample of generated images\n",
    "    \"\"\"\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "### Save checkpoints\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define a function which defines the training step within each epoch.\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    images -- training samples of the MNIST dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return ([gen_loss,disc_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    \"\"\"Ddefine a method which actually starts the training, saves and generate images for the dcgan model.\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    dataset -- the prepared dataset which shall be sampled from\n",
    "    epochs -- The defined hyperparameter epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for i,image_batch in enumerate(dataset):\n",
    "            loss_value_pair =train_step(image_batch)\n",
    "            if i % 20 == 0:\n",
    "                image_grid = generate_mnist_image_grid_for_dcgan(generator,title=\"Epoch {0}, iteration {1}\".format(epoch,i))                \n",
    "                save_generated_image(image_grid, epoch, i, \"./images/dcgan/generated_mnist_images_per_iteration\")\n",
    "                \n",
    "        # Save a generated image for every epoch\n",
    "        image_grid = generate_mnist_image_grid_for_dcgan(generator,title=\"Epoch {0}, iteration {1}\".format(epoch,i))                \n",
    "        save_generated_image(image_grid, epoch, i, \"./images/dcgan/generated_mnist_images_per_epoch\")\n",
    "        \n",
    "        # Save the model every 10 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))         \n",
    "        loss_values.append(loss_value_pair)\n",
    "    images_for_adcanced_evaluation = get_images_for_advanced_evaluation(generator,100)\n",
    "    print(images_for_adcanced_evaluation.shape)\n",
    "    np.save(\"./images/dcgan/images_for_advanced_evaluation\",images_for_adcanced_evaluation)\n",
    "    return loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly3UN0SLLY2l"
   },
   "outputs": [],
   "source": [
    "# Training Execution\n",
    "\n",
    "loss_values = train(train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "Restore the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload if sth. disapperas\n",
    "\n",
    "images_for_advanced_evaluation_dcgan = np.load(\"./images/dcgan/images_for_advanced_evaluation_by_dcgan.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "img = tf.keras.preprocessing.image.array_to_img(images_for_advanced_evaluation_dcgan[0])\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "\n",
    "### 3.2. Conditional Deep Convolutional Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    \"\"\"Using the Keras-API and Tensorlfow as Backend to define a Generator model for the cDCGAN\"\"\"\n",
    "    # Prepare noise input\n",
    "    input_z = tf.keras.layers.Input((100,))\n",
    "    dense_z_1 = tf.keras.layers.Dense(1024)(input_z)\n",
    "    act_z_1 = ACTIVATION(dense_z_1)\n",
    "    dense_z_2 = tf.keras.layers.Dense(128 * 7 * 7)(act_z_1)\n",
    "    bn_z_1 = tf.keras.layers.BatchNormalization()(dense_z_2)\n",
    "    reshape_z = tf.keras.layers.Reshape((7, 7, 128), input_shape=(128 * 7 * 7,))(bn_z_1)\n",
    "\n",
    "    # Prepare Conditional (label) input\n",
    "    input_c = tf.keras.layers.Input((100,))\n",
    "    dense_c_1 = tf.keras.layers.Dense(1024)(input_c)\n",
    "    act_c_1 = ACTIVATION(dense_c_1)\n",
    "    dense_c_2 = tf.keras.layers.Dense(128 * 7 * 7)(act_c_1)\n",
    "    bn_c_1 = tf.keras.layers.BatchNormalization()(dense_c_2)\n",
    "    reshape_c = tf.keras.layers.Reshape((7, 7, 128), input_shape=(128 * 7 * 7,))(bn_c_1)\n",
    "\n",
    "    # Combine input source\n",
    "    concat_z_c = tf.keras.layers.Concatenate()([reshape_z, reshape_c])\n",
    "\n",
    "    # Image generation with the concatenated inputs\n",
    "    up_1 = tf.keras.layers.UpSampling2D(size=(2, 2))(concat_z_c)\n",
    "    conv_1 = tf.keras.layers.Conv2D(64, (5, 5), padding='same')(up_1)\n",
    "    act_1 = ACTIVATION(conv_1)\n",
    "    up_2 = tf.keras.layers.UpSampling2D(size=(2, 2))(act_1)\n",
    "    conv_2 = tf.keras.layers.Conv2D(1, (5, 5), padding='same')(up_2)\n",
    "    act_2 = tf.keras.layers.Activation(\"tanh\")(conv_2)\n",
    "    model = tf.keras.Model(inputs=[input_z, input_c], outputs=act_2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generator:\")\n",
    "G = generator_model()\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    \"\"\"Using the Keras-API and Tensorlfow as Backend to define a Discriminatoer model for the cDCGAN\"\"\"\n",
    "    input_gen_image = tf.keras.layers.Input((28, 28, 1))\n",
    "    conv_1_image = tf.keras.layers.Conv2D(64, (5, 5), padding='same')(input_gen_image)\n",
    "    act_1_image = ACTIVATION(conv_1_image)\n",
    "    pool_1_image = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(act_1_image)\n",
    "    conv_2_image = tf.keras.layers.Conv2D(128, (5, 5))(pool_1_image)\n",
    "    act_2_image = ACTIVATION(conv_2_image)\n",
    "    pool_2_image = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(act_2_image)\n",
    "\n",
    "    input_c = tf.keras.layers.Input((100,))\n",
    "    dense_1_c = tf.keras.layers.Dense(1024)(input_c)\n",
    "    act_1_c = ACTIVATION(dense_1_c)\n",
    "    dense_2_c = tf.keras.layers.Dense(5 * 5 * 128)(act_1_c)\n",
    "    bn_c = tf.keras.layers.BatchNormalization()(dense_2_c)\n",
    "    reshaped_c = tf.keras.layers.Reshape((5, 5, 128))(bn_c)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate()([pool_2_image, reshaped_c])\n",
    "\n",
    "    flat = tf.keras.layers.Flatten()(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(1024)(flat)\n",
    "    act_1 = ACTIVATION(dense_1)\n",
    "    dense_2 = tf.keras.layers.Dense(1)(act_1)\n",
    "    act_2 = tf.keras.layers.Activation('sigmoid')(dense_2)\n",
    "    model = tf.keras.Model(inputs=[input_gen_image, input_c], outputs=act_2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discriminator:\")\n",
    "D = discriminator_model()\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(g, d):\n",
    "    \"\"\" Using the Keras-API and Tensorlfow as Backend to define a cDCGAN architecutre for \n",
    "        a cDCGAN which contaions two models (i) generator and (ii) discriminator\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    g -- Generator model \n",
    "    d -- Discriminator model\n",
    "    \"\"\"\n",
    "    \n",
    "    input_z = tf.keras.layers.Input((100,))\n",
    "    input_c = tf.keras.layers.Input((100,))\n",
    "    gen_image = g([input_z, input_c])\n",
    "    d.trainable = False\n",
    "    is_real = d([gen_image, input_c])\n",
    "    model = tf.keras.Model(inputs=[input_z, input_c], outputs=is_real)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined:\")\n",
    "GD = generator_containing_discriminator(G, D)\n",
    "GD.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "     \"\"\"Ddefine a method which actually starts the training, saves and generate images for the cDCGAN model.\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    epochs -- The defined hyperparameter epoch\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "\n",
    "    nb_of_iterations_per_epoch = int(train_images.shape[0] / batch_size)\n",
    "\n",
    "    print(\"Number of iterations per epoch: {0}\".format(nb_of_iterations_per_epoch))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #pbar = tqdm(desc=\"Epoch: {0}\".format(epoch), total=train_images.shape[0])\n",
    "\n",
    "        g_losses_for_epoch = []\n",
    "        d_losses_for_epoch = []\n",
    "\n",
    "        for i in range(nb_of_iterations_per_epoch):\n",
    "            noise = generate_noise((batch_size, 100))\n",
    "\n",
    "            image_batch = train_images[i * batch_size:(i + 1) * batch_size]\n",
    "            label_batch = train_labels[i * batch_size:(i + 1) * batch_size]        \n",
    "\n",
    "            generated_images = G.predict([noise, label_batch], verbose=0)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                image_grid = generate_mnist_image_grid(G,\n",
    "                                                                    title=\"Epoch {0}, iteration {1}\".format(epoch,\n",
    "                                                                                                            iteration))\n",
    "                save_generated_image(image_grid, epoch, i, \"./images/cdcgan/generated_mnist_images_per_iteration\")\n",
    "                #image_logger.log_images(\"generated_mnist_images_per_iteration\", [image_grid], iteration)\n",
    "\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * batch_size + [0] * batch_size\n",
    "            label_batches_for_discriminator = np.concatenate((label_batch, label_batch))\n",
    "            y = np.array(y)        \n",
    "\n",
    "            D_loss = D.train_on_batch([X, label_batches_for_discriminator], y)\n",
    "            d_losses_for_epoch.append(D_loss)\n",
    "\n",
    "            noise = generate_noise((batch_size, 100))\n",
    "            D.trainable = False\n",
    "            G_loss = GD.train_on_batch([noise, label_batch], [1] * batch_size)\n",
    "            D.trainable = True\n",
    "            g_losses_for_epoch.append(G_loss)        \n",
    "            #pbar.update(batch_size)\n",
    "            iteration += 1\n",
    "\n",
    "        # Save a generated image for every epoch\n",
    "        image_grid = generate_mnist_image_grid(G, title=\"Epoch {0}\".format(epoch))\n",
    "        save_generated_image(image_grid, epoch, 0, \"./images/cdcgan/generated_mnist_images_per_epoch\")    \n",
    "        #pbar.close()        \n",
    "        \n",
    "\n",
    "    images_for_adcanced_evaluation = get_images_for_advanced_evaluation(G,100)\n",
    "    print(images_for_adcanced_evaluation.shape)\n",
    "    np.save(\"./images/cdcgan/images_for_advanced_evaluation\",images_for_adcanced_evaluation)\n",
    "    G.save_weights(\"./models/cdcgan/cdcgan_generators.h5\")\n",
    "    D.save_weights(\"./models/cdcgan/cdcgan_discriminator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.save_weights(\"./models/cdcgan/cdcgan_generators.h5\")\n",
    "D.save_weights(\"./models/cdcgan/cdcgan_discriminator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_for_advanced_evaluation_cdcgan = get_images_for_advanced_evaluation_cdcgan(G,100)\n",
    "print(images_for_adcanced_evaluation_cdcgan.shape)\n",
    "np.save(\"./images/cdcgan/images_for_advanced_evaluation\",images_for_adcanced_evaluation_cdcgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.preprocessing.image.array_to_img(images_for_adcanced_evaluation_cdcgan[0])\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "\n",
    "## 4. Evaluation\n",
    "\n",
    "### 4.1. Visual Examination\n",
    "\n",
    " Use the generator model to get a set of images before training has started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator_model()\n",
    "D = discriminator_model()\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for i in range(10):\n",
    "    noise = generate_noise((10, 100))\n",
    "    label_input = generate_condition_embedding(6, 10)\n",
    "    gen_images = G.predict([noise, label_input], verbose=0)\n",
    "    generated_images.extend(gen_images)\n",
    "\n",
    "generated_images = np.array(generated_images)\n",
    "image_grid = combine_images(generated_images)\n",
    "image_grid = inverse_transform_images(image_grid)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(image_grid, cmap=\"gray\")\n",
    "#ax.set_title(title)\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the disciminator to predicted for the generated images whether the are  fake or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images.shape\n",
    "label_input = generate_condition_embedding(6, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = D.predict([generated_images.reshape((100,28,28,1)), label_input.reshape((100,100))])\n",
    "prediction.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trainned generator model to get a set of images before training has started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.load_weights(\"./models/cdcgan/cdcgan_generators.h5\")\n",
    "D.load_weights(\"./models/cdcgan/cdcgan_discriminator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "\n",
    "for i in range(10):\n",
    "    noise = generate_noise((10, 100))\n",
    "    label_input = generate_condition_embedding(6, 10)\n",
    "    gen_images = G.predict([noise, label_input], verbose=0)\n",
    "    generated_images.extend(gen_images)\n",
    "\n",
    "generated_images = np.array(generated_images)\n",
    "print(generated_images.shape)\n",
    "image_grid = combine_images(generated_images)\n",
    "image_grid = inverse_transform_images(image_grid)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(image_grid, cmap=\"gray\")\n",
    "#ax.set_title(title)\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the disciminator to predicted for the generated images whether the are  fake or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images.shape\n",
    "label_input = generate_condition_embedding(6, 100)\n",
    "prediction = D.predict([generated_images.reshape((100,28,28,1)), label_input.reshape((100,100))])\n",
    "prediction.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "\n",
    "### 4.2 K-Nearest-Neighbour (kNN)\n",
    "\n",
    "Another  approach for  summarizing generator performance is “Nearest Neighbors.” This involves selecting examples of real images from the domain and locating one or more most similar generated images for comparison.\n",
    "\n",
    "Distance measures, such as Euclidean distance between the image pixel data, is often used for selecting the most similar generated images.\n",
    "\n",
    "The nearest neighbor approach is useful to give context for evaluating how realistic the generated images happen to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_dcgan_images = images_for_advanced_evaluation_dcgan[[0,10,20,30,40,50,60,70,80,90]]\n",
    "fake_cdcgan_images = generated_images[[0,10,20,30,40,50,60,70,80,90]]\n",
    "\n",
    "real_images = train_images.reshape((train_images.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=1, ).fit(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(fake_dcgan_images.reshape((fake_dcgan_images.shape[0],-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_of_dcgan = train_images[indices.reshape(10)]\n",
    "img = np.concatenate((fake_dcgan_images, neighbors_of_dcgan), axis=0)\n",
    "\n",
    "# show imgs\n",
    "fig = plt.figure(figsize=(40, 8))\n",
    "\n",
    "title = ['DCGAN generated images', 'kNNs k=1','']\n",
    "\n",
    "for i in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n",
    "    plt.imshow(img[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    if i == 14:\n",
    "        title_index = 1\n",
    "    elif i == 4:\n",
    "        title_index = 0\n",
    "    else:\n",
    "        title_index = 2\n",
    "    ax.set_title(title[title_index],fontweight =\"bold\",fontsize=30)\n",
    "\n",
    "plt.savefig('DCGAN and KNN with k = 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(fake_cdcgan_images.reshape((fake_cdcgan_images.shape[0],-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_of_cdcgan = train_images[indices.reshape(10)]\n",
    "img = np.concatenate((fake_cdcgan_images, neighbors_of_cdcgan), axis=0)\n",
    "\n",
    "# show imgs\n",
    "fig = plt.figure(figsize=(40, 8))\n",
    "\n",
    "title = ['cDCGAN generated images', 'Result kNN with k=1','']\n",
    "\n",
    "for i in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n",
    "    plt.imshow(img[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    if i == 14:\n",
    "        title_index = 1\n",
    "    elif i == 4:\n",
    "        title_index = 0\n",
    "    else:\n",
    "        title_index = 2\n",
    "    ax.set_title(title[title_index],fontweight =\"bold\",fontsize=30)\n",
    "    \n",
    "plt.savefig('cDCGAN and KNN with k = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.3'></a>\n",
    "\n",
    "### Fréchet Inception Distance (FID)\n",
    "\n",
    "The Frechet Inception Distance, or FID, score was proposed and used by Martin Heusel, et al. in their 2017 paper titled “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” The score was proposed as an improvement over the existing Inception Score.\n",
    "\n",
    ">FID performs well in terms of discriminability, robustness and computational efficiency. […] It has been shown that FID is consistent with human judgments and is more robust to noise than IS — [Pros and Cons of GAN Evaluation Measures, 2018](https://arxiv.org/pdf/1802.03446.pdf).\n",
    "\n",
    "Like the inception score, the FID score uses the inception v3 model. Specifically, the coding layer of the model (the last pooling layer prior to the output classification of images) is used to capture computer vision specific features of an input image. These activations are calculated for a collection of real and generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_cdcgan = []\n",
    "\n",
    "for i in range(10):\n",
    "    noise = generate_noise((100, 100))\n",
    "    label_input = generate_condition_embedding(i, 100)\n",
    "    gen_images = G.predict([noise, label_input], verbose=0)\n",
    "    generated_images_cdcgan.extend(gen_images)\n",
    "\n",
    "generated_images_cdcgan = np.array(generated_images)\n",
    "print(generated_images_cdcgan.shape)\n",
    "image_grid = combine_images(generated_images_cdcgan)\n",
    "image_grid = inverse_transform_images(image_grid)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.axis(\"off\")\n",
    "ax.imshow(image_grid, cmap=\"gray\")\n",
    "#ax.set_title(title)\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_dcgan = np.load(\"./images/dcgan/images_for_advanced_evaluation_by_dcgan.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_train,y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 4444\n",
    "#plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "\n",
    "print(x_test.shape)\n",
    "pred = model.predict(x_test)\n",
    "print(pred.shape)\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = x_train[np.random.choice(x_train.shape[0], 1000, replace=False), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(model, real_images, generated_images):    \n",
    "    \"\"\"Uses the Keras-API and the Tensorflow backend to generat the so-called FID score.\n",
    "    \n",
    "    Keyword arguments:    \n",
    "    model-- Any binary classifier which can distinguish between two images\n",
    "    real_images -- images sampled from the training dataset\n",
    "    generated_images -- images sampled from the set of GAN generated_images\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate activations\n",
    "    act1 = model.predict(real_images)\n",
    "    act2 = model.predict(generated_images)\n",
    "\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\n",
    "    # calculate 8 of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "    # calculate fid\n",
    "fid = calculate_fid(model, real_images, real_images)\n",
    "print('Baseline FID score from real to real images: %.2f' % abs(fid))\n",
    "fid = calculate_fid(model, real_images, fake_dcgan_images)\n",
    "print('FID scroe from real and DCGAN generated images: %.2f' % fid)\n",
    "fid = calculate_fid(model, real_images, generated_images_cdcgan)\n",
    "print('FID scroe from real and cDCGAN generated images: %.2f' % fid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
